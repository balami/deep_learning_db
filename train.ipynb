{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nB2qKp60U9k"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.keras import layers\n",
    "import math,random\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "restore=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CARDINALITY = 6.56944320390689 #3710592\n",
    "MIN_CARDINALITY = 0.30102999566398 #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "dq9xmrBW0c3C",
    "outputId": "249949ec-f936-4d92-d2d1-905ae71ba709"
   },
   "outputs": [],
   "source": [
    "with open('encoded_job_onehot.json','r') as f:\n",
    "    data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D3hH4Fkq9kN5",
    "outputId": "eb069b59-7529-44e5-dcb8-471cf7a9e2dd"
   },
   "outputs": [],
   "source": [
    "split_random=False\n",
    "overfit_split=False\n",
    "\n",
    "keys = list(data_dict.keys())\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "if split_random:\n",
    "    \n",
    "    for key in keys:\n",
    "        if key not in [\"28c.sql\",\"17b.sql\",\"3a.sql\",\"19c.sql\",\"26c.sql\",\n",
    "                            \"15d.sql\",\"16d.sql\",\"16c.sql\",\"20c.sql\",\"15a.sql\",\n",
    "                            \"3c.sql\",\"20b.sql\",\"31b.sql\",\"25c.sql\",\"30b.sql\",\n",
    "                            \"8d.sql\",\"20a.sql\",\"19b.sql\",\"19d.sql\",\"17f.sql\",\n",
    "                            \"28a.sql\",\"6c.sql\",\"2b.sql\",\"7b.sql\",\"4a.sql\",\n",
    "                            \"9b.sql\",\"13d.sql\",\"27c.sql\",\"6d.sql\",\"7c.sql\",\n",
    "                            \"6f.sql\",\"33c.sql\",\"17c.sql\",\"23c.sql\",\"29b.sql\",\"1c.sql\",\"17d.sql\"]:\n",
    "            train_dict[key]=data_dict[key]\n",
    "        else:\n",
    "            test_dict[key]=data_dict[key]\n",
    "            \n",
    "elif overfit_split:\n",
    "    train_dict=data_dict\n",
    "    test_dict=data_dict\n",
    "\n",
    "else:\n",
    "    train_dict=dict(list(data_dict.items())[:70])\n",
    "    test_dict=dict(list(data_dict.items())[70:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "n1_num = 32\n",
    "n2_num = 128\n",
    "\n",
    "table_vlength =n2_num\n",
    "join_vlength=n2_num\n",
    "and_vlength=n2_num\n",
    "normalized_and_vlength=n2_num\n",
    "or_vlength=n2_num\n",
    "nested_or_vlength=n2_num\n",
    "\n",
    "# initializers = tf.contrib.layers.variance_scaling_initializer(\n",
    "#     factor=2.0,\n",
    "#     mode='FAN_IN',\n",
    "#     uniform=False,\n",
    "#     seed=42,\n",
    "#     dtype=tf.dtypes.float32)\n",
    "\n",
    "# initializers = tf.compat.v1.keras.initializers.he_uniform(seed=42)\n",
    "initializer = tf.initializers.GlorotUniform()\n",
    "\n",
    "# external_dropout=1\n",
    "# internal_dropout=1\n",
    "\n",
    "activation_fn = tf.nn.relu\n",
    "# activation_fn = tf.nn.leaky_relu\n",
    "# activation_fn=tf.nn.swish\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# learning_rate = 0.001\n",
    "# learning_rate = 0.0001\n",
    "\n",
    "# train_optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "# train_optimizer=tf.train.RMSPropOptimizer(learning_rate)\n",
    "# train_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.9, name='Momentum', use_nesterov=True)\n",
    "# train_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.9, name='Momentum', use_nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "vfMev-ZQ0jEO",
    "outputId": "f6541067-f6cc-42d5-9226-533c7ab97c3a"
   },
   "outputs": [],
   "source": [
    "temp_tables, temp_joins, temp_and, temp_normalized_and = ([] for i in range(4))\n",
    "tables_output, joins_output, and_output, normalized_and_output = ([] for i in range(4))\n",
    "\n",
    "cardinalities=[]\n",
    "target=[]\n",
    "\n",
    "def card_est_last_layers(network_type, tables_output, joins_output, and_output, normalized_and_output):\n",
    "    inp=tf.concat([tables_output, joins_output, and_output, normalized_and_output], axis=1)\n",
    "    net= tf.nn.dropout(slim.fully_connected(inp, 512, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)),  internal_dropout)\n",
    "    net= tf.nn.dropout(slim.fully_connected(net, 512, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)), internal_dropout)\n",
    "    cardinality = slim.fully_connected(net, 1, activation_fn=tf.nn.sigmoid)\n",
    "    return network_type(cardinality)\n",
    "\n",
    "def tables_layers(network_type, tables):\n",
    "    tables_net= tf.nn.dropout(slim.fully_connected(tables, n1_num, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)),internal_dropout)\n",
    "    table_vector = slim.fully_connected(tables_net, table_vlength,  activation_fn)\n",
    "    return network_type(table_vector)\n",
    "\n",
    "def joins_layers(network_type, joins):\n",
    "    joins_net= tf.nn.dropout(slim.fully_connected(joins, n1_num, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)),internal_dropout)\n",
    "    join_vector = slim.fully_connected(joins_net, join_vlength,   activation_fn)\n",
    "    return network_type(join_vector)\n",
    "\n",
    "def and_layers(network_type, ands):\n",
    "    and_net= tf.nn.dropout(slim.fully_connected(ands, n1_num, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)),internal_dropout)\n",
    "    and_vector = slim.fully_connected(and_net, and_vlength,   activation_fn)\n",
    "    return network_type(and_vector)\n",
    "\n",
    "def normalized_and_layers(network_type, normalized_ands):\n",
    "    normalized_and_net= tf.nn.dropout(slim.fully_connected(normalized_ands, n1_num, activation_fn,weights_initializer=initializers,weights_regularizer=slim.l2_regularizer(1e-8)),internal_dropout)\n",
    "    normalized_and_vector = slim.fully_connected(normalized_and_net, normalized_and_vlength,   activation_fn)\n",
    "    return network_type(normalized_and_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_device='/gpu:0'\n",
    "print(\"Device defined\")\n",
    "\n",
    "def _general_network_template(tables_outputs, joins_outputs, and_output, normalized_and_output):\n",
    "    return card_est_last_layers(collections.namedtuple('DS_network', ['values']), tables_outputs, joins_outputs, and_output, normalized_and_output)\n",
    "\n",
    "def _tables_network_template(tables):\n",
    "    return tables_layers(collections.namedtuple('tables_network', ['values']), tables)\n",
    "\n",
    "def _joins_network_template(joins):\n",
    "    return joins_layers(collections.namedtuple('joins_network', ['values']), joins)\n",
    "\n",
    "def _and_network_template(ands):\n",
    "    return and_layers(collections.namedtuple('and_network', ['values']), ands)\n",
    "\n",
    "def _normalized_and_network_template(normalized_ands):\n",
    "    return normalized_and_layers(collections.namedtuple('normalized_and_network', ['values']), normalized_ands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbgP-C0W08q1"
   },
   "outputs": [],
   "source": [
    "def _build_general_network():\n",
    "    global cardinalities\n",
    "    net= tf.make_template('network', _general_network_template)\n",
    "    cardinalities = net(tables_output, joins_output, and_output, normalized_and_output)\n",
    "    tf.summary.histogram(\"predicted cardinalities\", cardinalities)\n",
    "\n",
    "def _build_tables_network():\n",
    "    global tables_output\n",
    "    tables_net= tf.make_template('tables_network', _tables_network_template)\n",
    "    tables_output= tables_net(temp_tables)\n",
    "    tables_output=tf.reduce_sum(tables_output, 2)[0]\n",
    "\n",
    "def _build_joins_network():\n",
    "    global joins_output\n",
    "    joins_net= tf.make_template('joins_network', _joins_network_template)\n",
    "    joins_output= joins_net(temp_joins)\n",
    "    joins_output=tf.reduce_sum(joins_output, 2)[0]\n",
    "    \n",
    "def _build_and_network():\n",
    "    global and_output\n",
    "    and_net= tf.make_template('and_network', _and_network_template)\n",
    "    and_output= and_net(temp_and)\n",
    "    and_output=tf.reduce_sum(and_output, 2)[0]\n",
    "\n",
    "def _build_normalized_and_network():\n",
    "    global normalized_and_output\n",
    "    normalized_and_net= tf.make_template('normalized_and_network', _normalized_and_network_template)\n",
    "    normalized_and_output= normalized_and_net(temp_normalized_and)\n",
    "    normalized_and_output=tf.reduce_sum(normalized_and_output, 2)[0]\n",
    "\n",
    "# def _build_train_op():\n",
    "#     loss=tf.keras.losses.MSE(cardinalities[0],tf.reshape(target_ph, (batch_size, 1)))\n",
    "#     print_op=tf.print(tf.reduce_mean(loss))\n",
    "#     tf.summary.scalar('Loss', tf.reduce_mean(loss))\n",
    "#     with tf.control_dependencies([print_op]):\n",
    "#         a=optimizer.minimize(loss)\n",
    "#         return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "DspzTaIvM9TE",
    "outputId": "408deb03-e664-4b82-fa1b-2fdd2cefc925"
   },
   "outputs": [],
   "source": [
    "with tf.device(tf_device):  \n",
    "    cardinalities=tf.placeholder(tf.float32, name='cardinalities_ph')\n",
    "    target_ph=tf.placeholder(tf.float32, name='target')\n",
    "\n",
    "    it_ph=tf.placeholder(tf.float32, name='iteration')\n",
    "\n",
    "    internal_dropout = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "    tables_output=tf.placeholder(tf.float32, (None, table_vlength), name='tables_output_ph')\n",
    "    joins_output=tf.placeholder(tf.float32, (None, join_vlength), name='joins_output_ph')\n",
    "    and_output=tf.placeholder(tf.float32, (None, and_vlength), name='and_output_ph')\n",
    "    normalized_and_output=tf.placeholder(tf.float32, (None, normalized_and_vlength), name='normalized_and_output_ph')\n",
    "\n",
    "    temp_tables=tf.placeholder(tf.float32, (None, None, 21), name='temp_table_ph')\n",
    "    temp_joins=tf.placeholder(tf.float32, (None, None, 171), name='temp_join_ph') #Note the input size of the temp ones, which should match the size of items in the json\n",
    "    temp_and=tf.placeholder(tf.float32, (None, None, 391), name='temp_and_ph')\n",
    "    temp_normalized_and=tf.placeholder(tf.float32, (None, None, 92), name='temp_normalized_and_ph')\n",
    "\n",
    "    table_net= _build_tables_network()\n",
    "    join_net= _build_joins_network()\n",
    "    and_net= _build_and_network()\n",
    "    normalized_and_net= _build_normalized_and_network()\n",
    "    net= _build_general_network()\n",
    "#     _train_op = _build_train_op()\n",
    "\n",
    "    loss =tf.keras.losses.MSE(cardinalities[0],tf.reshape(target_ph, (batch_size, 1)))\n",
    "    train_loss = tf.summary.scalar('Train Loss', tf.reduce_mean(loss))\n",
    "\n",
    "    optimizer=train_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR=\"graphs/test/train\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "_sess = tf.Session('', config=config)\n",
    "merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, _sess.graph)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not restore:\n",
    "  _sess.run(init_op)\n",
    "else:\n",
    "  saver.restore(_sess, \"hyperparameters/test/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(query_batch):\n",
    "    q_names = query_batch\n",
    "    target = [data_dict[k]['Cardinality'] for k in query_batch]\n",
    "\n",
    "    all_tables = [data_dict[k]['Tables'] for k in query_batch]\n",
    "    tables_input = tf.keras.preprocessing.sequence.pad_sequences(all_tables,padding='post',dtype='float64')\n",
    "    tables_input = np.array(tables_input).tolist()\n",
    "\n",
    "    all_joins = [data_dict[k]['Joins'] for k in query_batch]\n",
    "    joins_input = tf.keras.preprocessing.sequence.pad_sequences(all_joins,padding='post',dtype='float64')\n",
    "    joins_input = np.array(joins_input).tolist()\n",
    "\n",
    "    all_ands = [data_dict[k]['Predicate String'] for k in query_batch]\n",
    "    and_input = tf.keras.preprocessing.sequence.pad_sequences(all_ands,padding='post',dtype='float64')\n",
    "    and_input = np.array(and_input).tolist()\n",
    "\n",
    "    all_norm_ands = [data_dict[k]['Predicate Numeric'] for k in query_batch]\n",
    "    normalized_and_input = tf.keras.preprocessing.sequence.pad_sequences(all_norm_ands,padding='post',dtype='float64')\n",
    "    normalized_and_input = np.array(normalized_and_input).tolist()\n",
    "    \n",
    "    return q_names,target,tables_input,joins_input,and_input,normalized_and_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_it = 3000\n",
    "num_epochs=4\n",
    "\n",
    "for num in range(total_it):\n",
    "    if num==total_it-1:\n",
    "        test_batch = np.array(list(test_dict.keys()))\n",
    "        test_q_names,test_target,test_tables_input,test_joins_input,test_and_input,test_normalized_and_input = get_inputs(test_batch)\n",
    "        clean_results=[]\n",
    "    else:\n",
    "        train_batch = np.random.choice(np.array(list(train_dict.keys())), batch_size, replace=True)\n",
    "        train_q_names,train_target,train_tables_input,train_joins_input,train_and_input,train_normalized_and_input = get_inputs(train_batch)\n",
    "\n",
    "    if num!=total_it-1:\n",
    "        for epoch in range(num_epochs):\n",
    "            result,summ =_sess.run([optimizer,merged], feed_dict={it_ph:num, target_ph:train_target,internal_dropout:1.0,\n",
    "            temp_tables:np.array(train_tables_input, dtype=np.float32), \n",
    "            temp_joins:np.array(train_joins_input, dtype=np.float32), \n",
    "            temp_and:np.array(train_and_input, dtype=np.float32),\n",
    "            temp_normalized_and:np.array(train_normalized_and_input, dtype=np.float32)\n",
    "            })\n",
    "            summary_writer.add_summary(summ,num)\n",
    "    if num==total_it-1:\n",
    "        [result] =_sess.run([cardinalities], feed_dict={it_ph:num, target_ph:test_target, \n",
    "            temp_tables:np.array(test_tables_input, dtype=np.float32), \n",
    "            temp_joins:np.array(test_joins_input, dtype=np.float32), \n",
    "            temp_and:np.array(test_and_input, dtype=np.float32), \n",
    "            temp_normalized_and:np.array(test_normalized_and_input, dtype=np.float32)\n",
    "            })\n",
    "        for it in range(len(result[0])):\n",
    "            clean_results.append(result[0][it][0])\n",
    "\n",
    "save_path = saver.save(_sess, \"hyperparameters/test/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LGW77aeA9YdM",
    "outputId": "ec284fcb-f9d2-4322-cd9d-8f3edbac633d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(test_target, clean_results)\n",
    "mean_absolute_error(test_target, clean_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_names = test_q_names\n",
    "temp_numbers = [re.findall('\\d+', query) for query in q_names]\n",
    "q_template_num = [ item for elem in temp_numbers for item in elem]\n",
    "target = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZAvSgocI_L6"
   },
   "outputs": [],
   "source": [
    "def denormalize_selectivity(cardinality_list):\n",
    "  denorm_sel=[]\n",
    "  for val in cardinality_list:\n",
    "    selectivity_val = (val*(MAX_CARDINALITY-MIN_CARDINALITY))+MIN_CARDINALITY\n",
    "    final_val = int(round(10**selectivity_val))\n",
    "    denorm_sel.append(final_val)\n",
    "  return denorm_sel\n",
    "\n",
    "denorm_pred_sel = denormalize_selectivity(clean_results)\n",
    "denorm_actual_sel = denormalize_selectivity(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "df_all['Query Number'] = q_names\n",
    "df_all['Target Cardinality'] = denorm_actual_sel\n",
    "df_all['Predicted Cardinality'] = denorm_pred_sel\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.to_csv('Target_vs_Predicted_cardinality_table.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame()\n",
    "df_target['Query Number'] = q_names\n",
    "df_target['Query Template Number'] = q_template_num\n",
    "df_target['Cardinality'] = denorm_actual_sel\n",
    "df_target['Labels'] = 'Target'\n",
    "df_target.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_predicted = pd.DataFrame()\n",
    "df_predicted['Query Number'] = q_names\n",
    "df_predicted['Query Template Number'] = q_template_num\n",
    "df_predicted['Cardinality'] = denorm_pred_sel\n",
    "df_predicted['Labels'] = 'Predicted'\n",
    "df_predicted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_linear = df_target.append(df_predicted,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatui = [\"#1192fb\",\"#fb1056\"]\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(x=\"Query Number\", y=\"Cardinality\", hue=\"Labels\", data=df_linear, palette=flatui)\n",
    "plt.ylabel(\"Cardinalities\", size=14)\n",
    "plt.xlabel(\"Query Numbers\", size=14)\n",
    "plt.xticks(rotation=90) \n",
    "plt.title(\"Target vs Predicted Cardinality\", size=18)\n",
    "plt.savefig(\"plots/test/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"linear_scale_all_prediction_bars.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_templates=df_linear['Query Template Number'].unique().tolist()\n",
    "\n",
    "for template in query_templates:\n",
    "    df_linear_templates = df_linear.loc[df_linear['Query Template Number']==template]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=\"Query Number\", y=\"Cardinality\", hue=\"Labels\", data=df_linear_templates, palette=flatui)\n",
    "    plt.ylabel(\"Cardinalities\", size=11)\n",
    "    plt.xlabel(\"Query Numbers\", size=11)\n",
    "    plt.title(\"Actual vs Predicted Cardinality\", size=11)\n",
    "    plt.savefig(\"plots/test/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+template+\"_linearscale_prediction_bars.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df_linear\n",
    "df_log['Cardinality'] = np.log(df_log['Cardinality'])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(x=\"Query Number\", y=\"Cardinality\", hue=\"Labels\", data=df_log, palette=flatui)\n",
    "plt.ylabel(\"Cardinalities\", size=14)\n",
    "plt.xlabel(\"Query Numbers\", size=14)\n",
    "plt.xticks(rotation=90) \n",
    "plt.title(\"Target vs Predicted Cardinality\", size=18)\n",
    "plt.savefig(\"plots/test/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"log_scale_all_prediction_bars.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_templates=df_log['Query Template Number'].unique().tolist()\n",
    "\n",
    "for template in query_templates:\n",
    "    df_log_templates = df_log.loc[df_log['Query Template Number']==template]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=\"Query Number\", y=\"Cardinality\", hue=\"Labels\", data=df_log_templates, palette=flatui)\n",
    "    plt.ylabel(\"Cardinalities\", size=11)\n",
    "    plt.xlabel(\"Query Numbers\", size=11)\n",
    "    plt.title(\"Actual vs Predicted Cardinality\", size=11)\n",
    "    plt.savefig(\"plots/test/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+template+\"_logscale_prediction_bars.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "37c9kYQMD4eS",
    "outputId": "a8d3ed5c-c3ae-4241-c09e-df9c97e79057"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "palette='BuGn'\n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "# print(vars)\n",
    "vars_vals = _sess.run(vars)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "plt_size_layer1=(24,10)\n",
    "plt_size_layer2 = (35,15)\n",
    "for var, val in zip(vars, vars_vals):\n",
    "    \n",
    "    if var.name==\"tables_network/fully_connected/weights:0\":\n",
    "        print(\"table input weights at Layer 1:\")\n",
    "        plt.figure(figsize=plt_size_layer1)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.4, vmax=0.5,robust=True,annot=True, fmt=\".1f\",square=True)\n",
    "        ax.set_ylabel('Table Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 1')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"annoted_heatmap_tables_layer1.png\")\n",
    "        plt.show()\n",
    "    if var.name==\"tables_network/fully_connected/weights:0\":\n",
    "        print(\"table input weights at Layer 1:\")\n",
    "        plt.figure(figsize=plt_size_layer1)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.3, vmax=0.3,robust=True,annot=False, fmt=\".1f\",square=True)\n",
    "        ax.set_ylabel('Table Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 1')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_tables_layer1.png\")\n",
    "        plt.show()\n",
    "    if var.name==\"tables_network/fully_connected_1/weights:0\":\n",
    "        print(\"table input weights at Layer 2:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=True)\n",
    "        ax.set_ylabel('Table Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 2')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_tables_layer2.png\")\n",
    "        plt.show()\n",
    "        \n",
    "    if var.name==\"joins_network/fully_connected/weights:0\":\n",
    "        print(\"joins input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer1)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=False)\n",
    "        ax.set_ylabel('Join Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 1')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_joins_layer1.png\")\n",
    "        plt.show()\n",
    "    if var.name==\"joins_network/fully_connected_1/weights:0\":\n",
    "        print(\"joins input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=True)\n",
    "        ax.set_ylabel('Join Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 2')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_joins_layer2.png\")\n",
    "        plt.show()\n",
    "         \n",
    "    if var.name==\"and_network/fully_connected/weights:0\":\n",
    "        print(\"and input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=False)\n",
    "        ax.set_ylabel('Predicate String Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 1')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_pred_str_layer1.png\")\n",
    "        plt.show()      \n",
    "    if var.name==\"and_network/fully_connected_1/weights:0\":\n",
    "        print(\"and input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=True)\n",
    "        ax.set_ylabel('Predicate String Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 2')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_pred_str_layer2.png\")\n",
    "        plt.show()\n",
    "               \n",
    "    if var.name==\"normalized_and_network/fully_connected/weights:0\":\n",
    "        print(\"normalized and input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer1)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=False)\n",
    "        ax.set_ylabel('Predicate Numeric Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 1')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_pred_num_layer1.png\")\n",
    "        plt.show()\n",
    "    if var.name==\"normalized_and_network/fully_connected_1/weights:0\":\n",
    "        print(\"normalized and input weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=True)\n",
    "        ax.set_ylabel('Predicate Numeric Sets Input')\n",
    "        ax.set_xlabel('Units of Hidden Layer 2')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"heatmap_pred_num_layer2.png\")\n",
    "        plt.show()\n",
    "       \n",
    "    if var.name==\"network/fully_connected/weights:0\":\n",
    "        print(\"overall concat weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=False)\n",
    "        ax.set_xlabel('Units of Hidden Layer 3')\n",
    "        ax.set_ylabel('Concatenated Inputs at Layer 3')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"concatenated_layer3.png\")\n",
    "        plt.show()\n",
    "    if var.name==\"network/fully_connected_1/weights:0\":\n",
    "        print(\"overall concat weights:\")\n",
    "        plt.figure(figsize=plt_size_layer2)\n",
    "        ax = sns.heatmap(val,cmap=palette,vmin=-0.05, vmax=0.05,robust=True,square=False)\n",
    "        ax.set_ylabel('Concatenated Inputs at Layer 4')\n",
    "        ax.set_xlabel('Units of Hidden Layer 4')\n",
    "        ax.figure.savefig(\"plots/heatmaps/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"concatenated_layer4.png\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Laxmi's Thesis-May 2020.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
